{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Гауссовские процессы\n",
    "\n",
    "Вы, наверняка, встрчеали раньше в курсах по машинному обучению такую модель, как гауссовские процессы. Это очень гибкая и мощная модель, несмотря на свою внешнюю простоту.\n",
    "\n",
    "Гауссовский процесс -- это случайный процесс $(X_t)_{t \\in \\mathbb{R}}$ такой, что любой вектор $X_{t_1},..,X_{t_k}$, состоящий из набора сечений этого процесса будет иметь гауссовское распределение. Для простоты ограничим себя конкретным случаем: индексное пространство $T = \\mathbb{R}$, и процесс принимает значения в $\\Xi = \\mathbb{R}$. Для того, чтобы построить такой процесс нам нужно задать\n",
    "1. Матожидание $m:\\mathbb{R} \\to \\mathbb{R}$;\n",
    "2. Ковариационную функцию $K: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$.\n",
    "Первый параметр задаёт матожидание для любого вектора сечений, а второй -- ковариационную матрицу.\n",
    "\n",
    "Существование такого процесса легко проверить с помощью теоремы Колмогорова о существовании(см. доску)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Важный вопрос состоит в том, как именно задавать ковариационную матрицу. \n",
    "\n",
    "**Определение.** Функцию $K: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$ назовём положительно полуопределённой, если для любого набора $t_1,..,t_k \\in T$ матрица $B=(K(t_i,t_j))_{i,j=1,..,k}$ положительно определена, то есть,\n",
    "$$\n",
    "\\forall x \\neq 0 \\in \\mathbb{R}^k  \\quad x^\\top B x \\geq 0.\n",
    "$$\n",
    "\n",
    "\n",
    "**Утверждение.** Функция $K: \\mathbb{R} \\times \\mathbb{R} \\to \\mathbb{R}$ является ковариационной функцией некоторого процесса тогда и только тогда, когда она является симметричной ($K(t,s)=K(s,t)$) и положительно полуопределённой.\n",
    "**Док-во** на доске.\n",
    "\n",
    "Первая простая функция с такими свойствами -- это \n",
    "$$\n",
    "K(t,s)= \\begin{cases} &\\sigma^2, &t=s,\\\\\n",
    "&0,&t\\neq s\n",
    "\\end{cases}\n",
    "$$\n",
    "с некоторым параметром $\\sigma \\in \\mathbb{R}$. Если задать матожидание $m(t)=0$ (любой константе), то мы получим гауссовский белый шум.\n",
    "\n",
    "\n",
    "## Имплементация\n",
    "\n",
    "В более общем случае мы хотели бы иметь модель гауссовского процесса для произвольной ковариационной функции (предположим, что пользователь всегда задаёт симметричную и положительно полуопределённую) и произвольной функции матожидания $m(t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcess:\n",
    "    '''\n",
    "        General class for Gaussian Processes taking values in R and indexed by R\n",
    "    '''\n",
    "\n",
    "    def __init__(self, mu, covFun):\n",
    "        '''\n",
    "            Input:\n",
    "            functionHandler mu -- function mu(t) mapping R --> R, expected value\n",
    "                must return vector if t is a vector (broadcasting-friendly)\n",
    "            functionHandler covFun -- function covFun(t,s) mapping R x R --> R, covariance function, symmetric and positive-semidefinite\n",
    "                must return matrix when arguments are vectors (broadcasting-friendly)\n",
    "        '''\n",
    "        self.mu = mu\n",
    "        self.covFun = covFun\n",
    "\n",
    "    def sampleTrajectories(self, ts=np.arange(0,1,1/20), nSamples=5):\n",
    "        '''\n",
    "            Samples nSamples trajectories of the gaussian process\n",
    "            Input\n",
    "            float[] ts -- index array (N,)\n",
    "            int nSamples -- number of sample trajectories to generate\n",
    "        '''\n",
    "        pass\n",
    "        #YOUR CODE\n",
    "        #return  gaussian vectors with mu=self.mu(ts) and cov=self.covFun, nSamples of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlotGaussianProcess(gpInstance,nSamples=5,ts=np.arange(0,1,1/20),ax=None,title=\"Trajectories of Gaussian Process\"):\n",
    "    '''\n",
    "    Plots nSamples trajectories on a chart\n",
    "    Input\n",
    "    GaussianProcess gpInstance -- GaussianProcess object to sample from\n",
    "    int nSamples -- number of trajectories to generate\n",
    "    float[] ts -- indices (N,)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    uncertainty = 1.96 * np.sqrt(np.diag(gpInstance.covFun(ts,ts)))\n",
    "    mu = gpInstance.mu(ts)\n",
    "\n",
    "    if(ax is None):\n",
    "        f,ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "    ax.grid()\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel(\"T\",fontsize=14)\n",
    "    ax.fill_between(ts, mu + uncertainty, mu - uncertainty, alpha=0.2)\n",
    "    ax.plot(ts, mu, label='Mean')\n",
    "\n",
    "    samples = gpInstance.sampleTrajectories(ts,nSamples)\n",
    "\n",
    "    ax.plot(ts, samples.T, linestyle='--') #transpose to satisfy matplotlib\n",
    "    \n",
    "    ax.legend()\n",
    "\n",
    "    if(ax is None):\n",
    "        return f,ax\n",
    "    else:\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma=0.1\n",
    "def muWhNoise(t):\n",
    "    pass\n",
    "    #YOUR CODE\n",
    "\n",
    "def covWhNoise(t,s):\n",
    "    pass\n",
    "    #return WhNoise covariance, be ready to t,s being vectors\n",
    "\n",
    "gpWhiteNoise = GaussianProcess(muWhNoise,covWhNoise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot, finally \n",
    "a=0\n",
    "b=5\n",
    "Nt=100\n",
    "h=(b-a)/Nt\n",
    "ts = np.arange(a,b+h/2,h)\n",
    "PlotGaussianProcess(gpWhiteNoise,ts=ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Белый шум -- полезный процесс, потому что он часто используется в разных моделях, но всё-таки очень простой.\n",
    "\n",
    "## Другие ковариационные функции\n",
    "\n",
    "Как можно строить более сложные процессы? Очевидно, что основную сложность здесь представляет форма ковариационной функции, но их можно делать из более простых.\n",
    "\n",
    "1. Сумма двух ковариационных функций $K_1+K_2$ тоже будет ковариационной функцией;\n",
    "2. Если $K$ -- ковариационная функция, то $\\alpha K$ для $\\alpha \\geq 0$ тоже ковариационная функция\n",
    "3. Произведение двух ковариационных функций $K_1 \\cdot K_2$ тоже будет ковариационной функцией (теорема Шура об Адамаровом произведении)\n",
    "\n",
    "Весь этот инструментарий поддерживается в Gpy и sklearn.\n",
    "\n",
    "Примеры ковариационных функций:\n",
    "1. Ковариационная функция белого шума \n",
    "$$\n",
    "K(t,s)= \\begin{cases} &\\sigma^2, &t=s,\\\\\n",
    "&0,&t\\neq s;\n",
    "\\end{cases}\n",
    "$$\n",
    "2. Линейная $K(t,s)=ts$;\n",
    "3. Квадратично-экспоненциальная $K(t,s)= \\exp\\left(-\\frac{(t-s)^2}{2l^2}\\right)$;\n",
    "4. Функция Орнштейна-Уленбека $K(t,s)= \\exp\\left(-\\frac{(\\vert t-s\\vert }{2l^2}\\right)$;\n",
    "5. ...много других вы можете найти, например, в документации sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# more examples\n",
    "#sqExp parameter\n",
    "sqExpL=0.15\n",
    "def mu0(t):\n",
    "    return 0*np.ones(t.shape) \n",
    "    #zero expectation\n",
    "\n",
    "def covSqExp(t,s):\n",
    "    pass\n",
    "    #return sqExponential covariance,  be ready to t,s being vectors\n",
    "\n",
    "gpSqExp = GaussianProcess(mu0,covSqExp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "b=5\n",
    "Nt=150\n",
    "h=(b-a)/Nt\n",
    "ts = np.arange(a,b+h/2,h)\n",
    "PlotGaussianProcess(gpSqExp,ts=ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стационарность\n",
    "\n",
    "Гауссовские процессы уникальны тем, что для них стационарность в узком смысле эквивалентна стационарности в широком смысле. Проверять часто проще второе.\n",
    "\n",
    "Какие процессы выше будут стационарными (при $m(t)=0$)?..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте попробуем сравнить стационарный и нестационарный процесс."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covLin(t,s):\n",
    "    pass\n",
    "    #return linear covariance function, be ready to t,s being vectors\n",
    "def covSq(t,s):\n",
    "    pass\n",
    "    #return (ts)^2 covariance function, be ready to t,s being  vectors\n",
    "\n",
    "gpLin = GaussianProcess(mu0,covLin)\n",
    "gpSq = GaussianProcess(mu0,covSq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "b=5\n",
    "Nt=150\n",
    "h=(b-a)/Nt\n",
    "ts = np.arange(a,b+h/2,h)\n",
    "\n",
    "f, (ax1,ax2,ax3,ax4) = plt.subplots(1,4, figsize=(24,6))\n",
    "\n",
    "PlotGaussianProcess(gpSqExp,ts=ts,title=\"SquareExponential(l=\"+str(sqExpL)+\")\",ax=ax1 )\n",
    "PlotGaussianProcess(gpWhiteNoise,ts=ts,title=\"WhiteNoise(sigma=\"+str(sigma)+\")\",ax=ax2 )\n",
    "PlotGaussianProcess(gpLin,ts=ts,title=\"Linear\",ax=ax3 )\n",
    "PlotGaussianProcess(gpSq,ts=ts,title=\"Square\",ax=ax4 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Интерполяция и подгонка параметров (машинное обучение)\n",
    "\n",
    "В обучении с учителем мы часто используем вероятностные модели $p_{\\boldsymbol\\theta}(\\mathbf{Y} \\lvert \\mathbf{X})$ для объяснения данных. Для нахождения $\\boldsymbol\\theta$ самым популярным методом остаётся [Метод Максимального правдоподобия](https://en.wikipedia.org/wiki/Maximum_likelihood_estimation).\n",
    "\n",
    "Гауссовские процессы, таким образом, могут выступать как байесовская модель. Вначале нужно объявить некоторое простое априорное распределение для параметра $\\boldsymbol\\theta$. Затем мы используем данные, чтобы уточнить параметры. Такой подход работает как для регрессии, так и для классификации.\n",
    "\n",
    "Вспомним, что гауссовский процесс -- это [случайный процесс](https://en.wikipedia.org/wiki/Stochastic_process), то есть, некоторое семейство случайных величин, проиндексированное  множеством $T$. Каждая точка $\\mathbf{x} \\in T$ переводится в случайную величину $f(\\mathbf{x})$, причём совместное распределение любого конечного числа точек $p(f(\\mathbf{x}_1),...,f(\\mathbf{x}_N))$ гауссовское:\n",
    "\n",
    "$$p(\\mathbf{f} \\lvert \\mathbf{X}) = \\mathcal{N}(\\mathbf{f} \\lvert \\boldsymbol\\mu, \\mathbf{K}).\\tag{1}$$\n",
    "\n",
    "В уравнении $(1)$ $\\mathbf{f} = (f(\\mathbf{x}_1),...,f(\\mathbf{x}_N))$, $\\boldsymbol\\mu = (m(\\mathbf{x}_1),...,m(\\mathbf{x}_N))$ и $K_{ij} = \\kappa(\\mathbf{x}_i,\\mathbf{x}_j)$. Здесь $m$ -- функция среднего и типично(но не обязательно) полагать $m(\\mathbf{x}) = 0$. Функция $\\kappa$ задаёт ковариационную функцию. Таким образом, гауссовский процесс можно понимать как некоторое распределение на функциях, задаваемое ковариационной функцией $\\mathbf{K}$ и функцией матожидания. \n",
    "\n",
    "Априорное распределение $p(\\mathbf{f} \\lvert \\mathbf{X})$ можно использовать для вычисления апостериорного $p(\\mathbf{f} \\lvert \\mathbf{X},\\mathbf{y})$ имея данные $\\mathbf{y}$. Апостериорное распределение можно использовать для предсказаний $\\mathbf{f}_*$ при данных $\\mathbf{X}_*$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "p(\\mathbf{f}_* \\lvert \\mathbf{X}_*,\\mathbf{X},\\mathbf{y}) \n",
    "&= \\int{p(\\mathbf{f}_* \\lvert \\mathbf{X}_*,\\mathbf{f})p(\\mathbf{f} \\lvert \\mathbf{X},\\mathbf{y})}\\ d\\mathbf{f} \\\\ \n",
    "&= \\mathcal{N}(\\mathbf{f}_* \\lvert \\boldsymbol{\\mu}_*, \\boldsymbol{\\Sigma}_*)\\tag{2}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Поскольку конечномерные распределения гауссовские, то вычислить апостериорное явно несложно: оно тоже будет гауссовским ([теорема о нормальной корреляции](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#Conditional_distributions)), как показывает уравнение $(2)$. Совместное распределение данных $\\mathbf{y}$ и предсказаний  $\\mathbf{f}_*$ записывается как\n",
    "\n",
    "$$\n",
    "\\begin{pmatrix}\\mathbf{y} \\\\ \\mathbf{f}_*\\end{pmatrix} \\sim \\mathcal{N}\n",
    "\\left(\\boldsymbol{0},\n",
    "\\begin{pmatrix}\\mathbf{K}_y & \\mathbf{K}_* \\\\ \\mathbf{K}_*^T & \\mathbf{K}_{**}\\end{pmatrix}\n",
    "\\right)\\tag{3}\n",
    "$$\n",
    "\n",
    "Если мы имеем $N$ элементов в данных для обучения и $N_*$ точек для предсказания, то $\\mathbf{K}_y = \\kappa(\\mathbf{X},\\mathbf{X}) + \\sigma_y^2\\mathbf{I} = \\mathbf{K} + \\sigma_y^2\\mathbf{I}$ есть матрица $N \\times N$, $\\mathbf{K}_* = \\kappa(\\mathbf{X},\\mathbf{X}_*)$ -- это матрица $N \\times N_*$ и $\\mathbf{K}_{**} = \\kappa(\\mathbf{X}_*,\\mathbf{X}_*)$ будет размера $N_* \\times N_*$. Параметр $\\sigma_y^2$ в $\\mathbf{K_y}$ описывает шум. Он задаётся нулём, если данные незашумлённые (редко, но бывает) и больше нуля в противном случае. Для простоты положим $\\boldsymbol{0}$. Статистики $\\boldsymbol{\\mu}_*$ и $\\boldsymbol{\\Sigma}_*$, можно вычислить<sup>[1][3]</sup>\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\boldsymbol{\\mu_*} &= \\mathbf{K}_*^T \\mathbf{K}_y^{-1} \\mathbf{y},\\tag{4,5} \\\\\n",
    "\\boldsymbol{\\Sigma_*} &= \\mathbf{K}_{**} - \\mathbf{K}_*^T \\mathbf{K}_y^{-1} \\mathbf{K}_*.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Но и это ещё не всё! Поскольку мы смоделировали распределение вектора данных, мы можем с помощью метода максимального правдоподобия подогнать параметры: как параметры функции матожидания, так и параметры ковариационной функции. С использованием различных методов оптимизации. Поскольку это сюжет для других курсов, здесь мы его подробно не рассматриваем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Решение интерполяции и аппроксимации при фиксированных параметрах\n",
    "\n",
    "Обе задачи в контексте гауссовских процессов решаются одинаково, однако в первом случае мы предполагаем, что шума в данных нет -- то есть, в точках данных $t_k$ нет никакой неопределённости насчёт значения $X_{t_k}$. Зато она есть в остальных точках.\n",
    "\n",
    "Сначала предположим, что параметры модели фиксированы, а данные не зашумлены. Таким образом, нам нужно решить задачу интерполяции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GaussianProcessPredictive(GaussianProcess):\n",
    "        \n",
    "    def predict(self, ts, tsData, xData):\n",
    "        '''\n",
    "        Computes the suffifient statistics of the GP posterior predictive distribution \n",
    "        from m training data X_train and Y_train and n new inputs X_s.\n",
    "    \n",
    "        Input\n",
    "            float[][] ts -- New input locations (n,d).\n",
    "            tsData -- Given locations (m,d).\n",
    "            xData -- Values at given locations (m,1).\n",
    "    \n",
    "        Returns:\n",
    "            Posterior mean vector (n,d) and covariance matrix (n,n).\n",
    "        '''\n",
    "        #YOUR CODE\n",
    "        #K = \n",
    "        #Ks = \n",
    "        #Kss = \n",
    "        \n",
    "        #Kinv = inversed K #!!\n",
    "    \n",
    "        # Equation (4)\n",
    "        #muS = \n",
    "\n",
    "        # Equation (5)\n",
    "        #covS = \n",
    "    \n",
    "        #return muS, covS\n",
    "        pass\n",
    "\n",
    "def PlotGaussianProcessPrediction(tsData,xsData, ts, musPred, covPred, nSamples=5, title=\"Interpolation with Gaussian Process\", ax=None):\n",
    "    '''\n",
    "    Plots nSamples trajectories on a chart\n",
    "    Input\n",
    "    float[] tsData -- data indices (N,)\n",
    "    float[] gpInstance -- GaussianProcess object to sample from\n",
    "    int nSamples -- number of trajectories to generate\n",
    "    float[] ts -- indices (N,)\n",
    "\n",
    "    '''\n",
    "    \n",
    "    uncertainty = 1.96 * np.sqrt(np.diag(covPred))\n",
    "\n",
    "    if(ax is None):\n",
    "        f,ax = plt.subplots(figsize=(7,7))\n",
    "\n",
    "    ax.grid()\n",
    "    ax.set_title(title, fontsize=16)\n",
    "    ax.set_xlabel(\"T\",fontsize=14)\n",
    "    ax.fill_between(ts, musPred + uncertainty, musPred - uncertainty, alpha=0.2)\n",
    "    ax.plot(ts, musPred, label='Mean')\n",
    "\n",
    "    samples = np.random.multivariate_normal(musPred,covPred,size=nSamples)\n",
    "\n",
    "    ax.plot(ts, samples.T, linestyle='--', linewidth=0.7) #transpose to satisfy matplotlib\n",
    "    \n",
    "    ax.scatter(tsData,xsData,marker=\"x\", c=\"red\", s=100)\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    if(ax is None):\n",
    "        return f,ax\n",
    "    else:\n",
    "        return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise free training data\n",
    "tsData = np.array([-4, -3, -2, -1, 1])\n",
    "xsData = np.sin(tsData)\n",
    "\n",
    "#new points\n",
    "a=-4\n",
    "b=1\n",
    "Nt=150\n",
    "h=(b-a)/Nt\n",
    "ts=np.arange(a,b+h/2,h)\n",
    "\n",
    "nSamples=5\n",
    "\n",
    "# Compute mean and covariance of the posterior predictive distribution\n",
    "gpSqExpPred = GaussianProcessPredictive(mu0,covSqExp)\n",
    "muS, covS = gpSqExpPred.predict(ts, tsData, xsData)\n",
    "\n",
    "PlotGaussianProcessPrediction(tsData,xsData, ts, musPred=muS, covPred=covS, nSamples=nSamples, title=\"Interpolation with SqExp(l=\"+str(sqExpL)+\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CORRECT PARAMETERS\n",
    "# Noise free training data\n",
    "ts2Data = np.array([-4, -3, -2, -1, 1])\n",
    "\n",
    "#new points\n",
    "a=-4\n",
    "b=1\n",
    "Nt=150\n",
    "h=(b-a)/Nt\n",
    "ts=np.arange(a,b+h/2,h)\n",
    "\n",
    "nSamples=5\n",
    "gpSqExpPred = GaussianProcessPredictive(mu0,covSqExp)\n",
    "\n",
    "# Sample data from the true process distribution\n",
    "xs2Data = gpSqExpPred.sampleTrajectories(ts2Data,nSamples=1).squeeze()\n",
    "# Compute mean and covariance of the posterior predictive distribution\n",
    "muS, covS = gpSqExpPred.predict(ts, ts2Data, xs2Data)\n",
    "\n",
    "PlotGaussianProcessPrediction(ts2Data,xs2Data, ts, musPred=muS, covPred=covS, nSamples=nSamples, title=\"Interpolation with SqExp(l=\"+str(sqExpL)+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видно, что среднее решает задачу интерполяции по заданным точкам, но неопределённость очень большая. Это происходит из-за того, что параметры процесса фиксированны, их можно подогнать, чтобы уменьшить неопределённость. Мы не будем отдельно обсуждать этого вопроса, но вы можете подробнее узнать об этом в других ресурсах.\n",
    "\n",
    "Зашумлённый случай требует лишь добавления белого шума."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqExpL=0.15\n",
    "sigma=0.6\n",
    "def mu0(t):\n",
    "    return 0*np.ones(t.shape) \n",
    "def covSqExp(t,s):\n",
    "    pass\n",
    "    #YOUR CODE\n",
    "\n",
    "def covWhNoise(t,s):\n",
    "    pass\n",
    "    #ok, now be ready also to the fact that t.shape[0] != s.shape[0]\n",
    "    \n",
    "def covFunSqExpWhNoise(t,s):\n",
    "    pass\n",
    "    #so?..\n",
    "\n",
    "gpSqExpWhNoisePred = GaussianProcessPredictive(mu0,covFunSqExpWhNoise)\n",
    "muS, covS = gpSqExpWhNoisePred.predict(ts, tsData, xsData)\n",
    "\n",
    "PlotGaussianProcessPrediction(tsData,xsData, ts, musPred=muS, covPred=covS, nSamples=nSamples, title=r\"Interpolation with SqExp(l=\"+str(sqExpL)+\") and WhNoise($\\sigma$=\"+str(sigma)+\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В завершение давайте попробуем посмотреть, как разные параметры влияют на модель."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = [\n",
    "    (0.1, 1.0),\n",
    "    (3.0, 1.0),\n",
    "    (1.0, 0.3),\n",
    "    (1.0, 3.0),\n",
    "]\n",
    "\n",
    "f, axs = plt.subplots(1, len(params),figsize=(22,7), sharey=True)\n",
    "\n",
    "for i in np.arange(len(params)):\n",
    "\n",
    "    sqExpL = params[i][0]\n",
    "    sigma = params[i][1] \n",
    "\n",
    "    #re-init of GP\n",
    "    def mu0(t):\n",
    "        return 0*np.ones(t.shape) \n",
    "    def covSqExp(t,s):\n",
    "        pass\n",
    "        #insert here your code from above\n",
    "\n",
    "    def covWhNoise(t,s):\n",
    "        pass\n",
    "        #insert here your code from above\n",
    "    \n",
    "    def covFunSqExpWhNoise(t,s):\n",
    "        pass\n",
    "        #insert here your code from above\n",
    "\n",
    "    gpSqExpWhNoisePred = GaussianProcessPredictive(mu0,covFunSqExpWhNoise)\n",
    "    ###\n",
    "\n",
    "\n",
    "    muS, covS = gpSqExpWhNoisePred.predict(ts, tsData, xsData)\n",
    "    PlotGaussianProcessPrediction(tsData, xsData, ts, musPred=muS, covPred=covS,nSamples=3,title=r\"$\\sigma=\"+str(sigma)+\"$, $l=\"+ str(sqExpL)+\"$\", ax=axs[i])\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f433b438e9f6b7f7de632f64020d80d9c5c579cb181b21714db0b763cdb1b505"
  },
  "kernelspec": {
   "display_name": "Python 3.6.2 64-bit ('py36': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
